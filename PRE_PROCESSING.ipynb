{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP3w4nYWDVfq"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou820PNHDX8X"
      },
      "source": [
        "This notebook contains the code for the pre-processing steps that were followed for the task of Honk Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usgYKMn1EEZN"
      },
      "source": [
        "# Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvQhmBkbEfuW"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import h5py\n",
        "import librosa\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdNITEcIEmVn"
      },
      "source": [
        "# Set Default Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Dm5dJiEz4t"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cMHOBdYEpoV"
      },
      "source": [
        "TRACK_DURATION = 2270  # measured in seconds; chosen because smallest track duration is 37:52 minutes which is 2272 seconds\n",
        "SEGMENT_DURATION = 1  # duration of audio for which spectrogram image must be made; measured in seconds\n",
        "N_FFT = 256  # corresponds to window size for FFT; used to adjust frequency resolution\n",
        "WIN_LENGTH = 192  # corresponds to number of samples in 24ms time interval at 8kHz\n",
        "IMAGE_SHAPE = (224,224)  # default size of images that are required\n",
        "N_RANDOM_SAMPLES = 1360 # number of samples to be selected randomly to balance the dataset\n",
        "\n",
        "LABEL_DATA_PATH = 'drive/MyDrive/<path-of-label-data>'  # path of the folder from where labels will be read\n",
        "AUDIO_DATA_PATH = 'drive/MyDrive/<path-of-audio-data>'  # path of the folder where the audio files are present\n",
        "DATASET_FILE_PATH = 'drive/MyDrive/<path-of-dataset>'  # path of the file that must be created"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFZURe2qFHkY"
      },
      "source": [
        "# Important Functions Used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yssbgP20FKkE"
      },
      "source": [
        "The following function takes a spectrogram array and processes it to convert to an 8-bit image.\n",
        "\n",
        "First we resize the image to the required size, followed by MinMax Scaling to get the values in the range [0,255]. Then the image is flipped over and inverted so that the black areas represent more energy regions. Finally, the greyscale image is converted to RGB by adding the same pixel intensity to all the three channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr0Pn_0OFWUk"
      },
      "source": [
        "# convert raw spectrogram data to 8-bit image\n",
        "def convert_spectrogram_to_image(spectrogram_data, image_shape):\n",
        "\n",
        "    # resize spectrogram data to obtain array of shape specified in parameter\n",
        "    resized_image = resize(spectrogram_data, image_shape)\n",
        "    \n",
        "    # scale spectrogram values to fit within 8-bit range to visualise as image\n",
        "    scaler = MinMaxScaler(feature_range=(0,255))\n",
        "    scaled_image = scaler.fit_transform(resized_image).astype(np.uint8)\n",
        "    \n",
        "    greyscale_image = np.flip(scaled_image, axis=0)  # put low frequencies at the bottom in image\n",
        "    greyscale_image = 255 - greyscale_image  # invert to make black ==> more energy\n",
        "\n",
        "    # convert greyscale image to RGB by adding same value to all three channels\n",
        "    rgb_image = np.asarray(np.dstack((greyscale_image, greyscale_image, greyscale_image)), dtype=np.uint8)\n",
        "\n",
        "    return rgb_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76VlzJNAF0fl"
      },
      "source": [
        "The next function takes as input the audio signal and various parameters used for Short-Time Fourier Transform and returns a list of images, each obtained by converting the corresponding segment of the spectrogram to greyscale image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk76Z3MYF41c"
      },
      "source": [
        "def convert_audio_to_spectrogram_images(signal, sample_rate, segment_duration=SEGMENT_DURATION, n_fft=N_FFT, win_length=WIN_LENGTH, image_shape=IMAGE_SHAPE):\n",
        "    # expected number of images we are expecting to extract from the audio file\n",
        "    images_per_track = int(librosa.get_duration(signal, sr=sample_rate) / segment_duration)\n",
        "    images = list()\n",
        "\n",
        "    # iterate for each count of image\n",
        "    for i in range(images_per_track):\n",
        "        start = i * segment_duration * sample_rate\n",
        "        end = (i + 1) * segment_duration * sample_rate\n",
        "\n",
        "        # calculate the spectrogram and convert from amplitude to log-scaled decibel values\n",
        "        short_time_fourier_transform = librosa.stft(signal[start:end], n_fft=n_fft, win_length=win_length)\n",
        "        spectrogram = librosa.amplitude_to_db(abs(short_time_fourier_transform))\n",
        "\n",
        "        # convert raw spectrogram data to 8-bit image\n",
        "        final_image = convert_spectrogram_to_image(spectrogram, image_shape)\n",
        "        \n",
        "        # add processed image to list\n",
        "        images.append(final_image)\n",
        "    \n",
        "    # return list containing all extracted images\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i3rxgIgF7hO"
      },
      "source": [
        "Perform complete preprocessing pipeline by doing the following steps:\n",
        "1. Get names of all audio files from the input data path\n",
        "2. Create .h5 dataset file\n",
        "3. Read each audio file individually and generate images of spectrograms of each 1 second audio window\n",
        "4. Try and obtain existing labels, or label files in a general way\n",
        "5. Append the image and label data to the dataset file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUQN4YVuGBn0"
      },
      "source": [
        "def preprocess(audio_path, labelled_data_path, output_dataset_path):\n",
        "\n",
        "    # get all audio filenames from audio_path folder\n",
        "    files = [f for f in os.listdir(audio_path) if os.path.isfile(os.path.join(audio_path, f))]\n",
        "\n",
        "    # create new h5 file to store dataset consisting of all the image data and the corresponding labels\n",
        "    dataset_file = h5py.File(output_dataset_path, 'a')\n",
        "\n",
        "    # iterate among all audio files found\n",
        "    for index, audio_file in enumerate(files):\n",
        "        \n",
        "        # get absolute path of the audio file\n",
        "        path_to_audio_file = os.path.join(audio_path, audio_file)\n",
        "\n",
        "        # load segment of file for training data\n",
        "        audio_signal, sample_rate = librosa.load(path_to_audio_file,\n",
        "                                                 mono=True,\n",
        "                                                 sr=None,\n",
        "                                                 duration=TRACK_DURATION)\n",
        "\n",
        "        # get the corresponding spectrogram images of the audio file\n",
        "        spectrogram_images = convert_audio_to_spectrogram_images(audio_signal, sample_rate=sample_rate)\n",
        "        \n",
        "        # check labelled_data_path to see if label information is present for current file or not\n",
        "        possible_file_name = audio_file.split('.')[0] + '.json'\n",
        "        possible_file_path = os.path.join(labelled_data_path, possible_file_name)\n",
        "        \n",
        "        # list containing label values for current audio file\n",
        "        labels = list()\n",
        "\n",
        "        # if label information exists, collect labels values to list\n",
        "        if os.path.isfile(possible_file_path):\n",
        "            with open(possible_file_path, 'r') as fp:\n",
        "                label_data = json.load(fp)\n",
        "                labels.extend(list(label_data.values())[:TRACK_DURATION])\n",
        "        \n",
        "        # else give default label value '2' to all samples\n",
        "        else:\n",
        "            labels.extend([2 for _ in range(len(spectrogram_images))])\n",
        "        \n",
        "        # convert lists to numpy arrays for faster processing\n",
        "        spectrogram_images = np.array(spectrogram_images, dtype=np.uint8)\n",
        "        labels = np.array(labels, dtype=np.uint8)\n",
        "\n",
        "        # extract Unknown labels for random undersampling\n",
        "        labels_uk = labels[ labels == 2 ]\n",
        "\n",
        "        # random indices to choose 25% of Unknown samples\n",
        "        \"\"\"\n",
        "        This is required because there are a lot of Unknown samples ~ 45,000 which are being stored uselessly now since they will be \n",
        "        randomly Undersampled during Balancing. To reduce the load later, we undersample these during storage itself.\n",
        "        \"\"\"\n",
        "        random_indices = np.random.randint(0, labels_uk.shape[0], int(0.25 * labels_uk.shape[0]))\n",
        "\n",
        "        # randomly undersample the Unknown class\n",
        "        images_uk = spectrogram_images[ labels == 2 ][random_indices]\n",
        "        labels_uk = labels_uk[ random_indices ]\n",
        "        \n",
        "        # extract the Known samples and labels\n",
        "        spectrogram_images = spectrogram_images[ labels != 2 ]\n",
        "        labels = labels[ labels != 2 ]\n",
        "\n",
        "        # add the selected Unknown samples to the Known samples\n",
        "        spectrogram_images = np.append(spectrogram_images, images_uk, axis=0)\n",
        "        labels = np.append(labels, labels_uk)\n",
        "\n",
        "        # dataset must be created only for the first time, remaining iterations only extend it further by adding more rows\n",
        "        if 'X' not in dataset_file.keys():\n",
        "            dataset_file.create_dataset('X', data=spectrogram_images, chunks=spectrogram_images.shape, maxshape=(None, None, None, None), dtype=np.uint8)\n",
        "            dataset_file.create_dataset('y', data=labels, chunks=(labels.shape[0], ), maxshape=(None, ), dtype=np.uint8)\n",
        "        \n",
        "        else:\n",
        "            # increase rows in existing array by the number of rows to be added in current iteration by resizing original array\n",
        "            dataset_file['X'].resize(dataset_file['X'].shape[0] + spectrogram_images.shape[0], axis=0)\n",
        "\n",
        "            # then add the new rows in the space that is newly generated\n",
        "            dataset_file['X'][-spectrogram_images.shape[0]:] = spectrogram_images\n",
        "\n",
        "            # append labels obtained from either JSON file or automatically generated to the overall dataset file\n",
        "            dataset_file['y'].resize(dataset_file['y'].shape[0] + labels.shape[0], axis=0)\n",
        "            dataset_file['y'][-labels.shape[0]:] = labels\n",
        "\n",
        "        # show completion message\n",
        "        print(f'{audio_file} has been processed successfully!')\n",
        "\n",
        "    # close dataset file and exit\n",
        "    dataset_file.close()\n",
        "    print(\"Completed!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE0MuaphGH6_"
      },
      "source": [
        "# Balancing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9McHZCL2GMBE"
      },
      "source": [
        "This section contains the code that is used to balance the classes in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuGuH0PGQrN"
      },
      "source": [
        "We first separate the dataset obtained after all the pre-processing steps are completed into separate arrays depending upon their labels. We then randomly select a fixed number of samples from all the classes and combine them together to create the final dataset. We combining, we add 25% of the known honks to the dataset as UNKNOWN samples to maintain the distribution in the dataset. Finally, we take the train-test split so that the performance of the model can be evaluated on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4EBRRgiG7gh"
      },
      "source": [
        "All the obtained arrays are stored to the dataset file, and the operation is completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb2W9fmVHACY"
      },
      "source": [
        "# function to randomly select samples from the dataset and to introduce known honks to balance the\n",
        "# huge class difference present in the dataset\n",
        "def balance_dataset(dataset_file_path):\n",
        "    \n",
        "    # open dataset file in reading mode and read datasets\n",
        "    dataset_file = h5py.File(dataset_file_path, 'r')\n",
        "    \n",
        "    # count the number of different labels in the dataset\n",
        "    # so that upper limit of sample count is known during selection of random samples\n",
        "    counter = Counter(dataset_file['y'])\n",
        "    num_no_honks = counter[0]\n",
        "    num_honks = counter[1]\n",
        "    num_uk = counter[2]\n",
        "\n",
        "    # generate random indices to select non-honk samples\n",
        "    random_indices_no_honk = np.random.randint(0, num_no_honks, int(0.5 * N_RANDOM_SAMPLES))\n",
        "\n",
        "    # generate random indices to select honk samples\n",
        "    random_indices_honk = np.random.randint(0, num_honks, int(0.5 * N_RANDOM_SAMPLES))\n",
        "    \n",
        "    # generate random indices to select unknown samples\n",
        "    random_indices_uk = np.random.randint(0, num_uk, N_RANDOM_SAMPLES)\n",
        "\n",
        "    # read the dataset\n",
        "    X = dataset_file['X'][()]\n",
        "    y = dataset_file['y'][()]\n",
        "\n",
        "    X_no_honk = X[ y == 0 ][ random_indices_no_honk ]   # choose random non-honk samples\n",
        "    X_honk = X[ y == 1 ][ random_indices_honk ] # choose random honk samples\n",
        "    X_uk = X[ y == 2 ][ random_indices_uk ] # choose random unknown samples\n",
        "\n",
        "    y_no_honk = np.zeros(X_no_honk.shape[0], dtype=y.dtype) # labels for non-honk samples\n",
        "    y_honk = np.ones(X_honk.shape[0], dtype=y.dtype)    # labels for honk samples\n",
        "    y_uk = np.full(X_uk.shape[0], 2, dtype=y.dtype) # labels for unknown samples\n",
        "\n",
        "    # delete original dataset file\n",
        "    os.remove(dataset_file_path)\n",
        "\n",
        "    # read the dataset file in write mode, overwriting over previous data\n",
        "    dataset_file = h5py.File(dataset_file_path, 'w')\n",
        "\n",
        "    # generate the final image dataset\n",
        "    X_final = np.append(X_honk, X_no_honk, axis=0)\n",
        "    y_final = np.append(y_honk, y_no_honk)\n",
        "\n",
        "    # make train and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42, stratify=y_final)\n",
        "\n",
        "    # choose some instances of honks in training dataset to add to Unknown class\n",
        "    num_honks_train = X_train[ y_train == 1 ].shape[0]  # number of honks in training dataset\n",
        "    random_indices_honk_add = np.random.randint(0, num_honks_train, int(0.25 * num_honks_train))\n",
        "\n",
        "    # extract chosen samples and labels\n",
        "    X_honk_add = X_train[ y_train == 1 ][random_indices_honk_add]\n",
        "    y_honk_add = np.full(X_honk_add.shape[0], 2, dtype=y.dtype)\n",
        "\n",
        "    # add some known honks to Unknown class\n",
        "    X_uk = np.append(X_uk, X_honk_add, axis=0)\n",
        "    y_uk = np.append(y_uk, y_honk_add)\n",
        "\n",
        "    # generate final image dataset\n",
        "    X_train = np.append(X_train, X_uk, axis=0)\n",
        "    y_train = np.append(y_train, y_uk)\n",
        "    \n",
        "    # create new datasets containing fresh training data\n",
        "    dataset_file.create_dataset(name='X_train', data=X_train, dtype=np.uint8)\n",
        "    dataset_file.create_dataset(name='y_train', data=y_train, dtype=np.uint8)\n",
        "\n",
        "    # create new datasets containing testing data\n",
        "    dataset_file.create_dataset(name='X_test', data=X_test, dtype=np.uint8)\n",
        "    dataset_file.create_dataset(name='y_test', data=y_test, dtype=np.uint8)\n",
        "    \n",
        "    # close dataset file\n",
        "    dataset_file.close()\n",
        "\n",
        "    # show confirmation\n",
        "    print(\"Dataset was balanced successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvI8r-C0HDzv"
      },
      "source": [
        "# Executing the Code (main)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_83ZKBiHG_1"
      },
      "source": [
        "# create training dataset\n",
        "    preprocess(AUDIO_DATA_PATH, LABEL_DATA_PATH, DATASET_FILE_PATH)\n",
        "\n",
        "    # balance the dataset by randomly undersampling majority class\n",
        "    # and inserting known instances of minority class in the dataset\n",
        "    balance_dataset(DATASET_FILE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR1hi1ubHTA1"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8vv-cT1HV19"
      },
      "source": [
        "These are some utility functions that can be used to check if the pre-processing steps worked as expected, or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBIbSY7sHczS"
      },
      "source": [
        "In the first function, we read the created dataset and print the distribution of the different classes in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipt1V8QiHicr"
      },
      "source": [
        "file = h5py.File(DATASET_FILE_PATH, 'r')\n",
        "counter1 = Counter(file['y_train'])\n",
        "counter2 = Counter(file['y_test'])\n",
        "print(f'Counter1: {counter1} ||| Counter2: {counter2}')\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUSQoSerHlGE"
      },
      "source": [
        "In the final function, we read the dataset and print the shapes of all the datasets. The number of samples in X_train should be equal to the number of labels in y_train. Similarly, for X_test and y_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnGwlgIfHxlz"
      },
      "source": [
        "file = h5py.File(DATASET_FILE_PATH, 'r')\n",
        "X_train = file['X_train']\n",
        "y_train = file['y_train']\n",
        "X_test = file['X_test']\n",
        "y_test = file['y_test']\n",
        "\n",
        "print(f'X_train: {X_train.shape}')\n",
        "print(f'y_train: {y_train.shape}')\n",
        "print(f'X_test: {X_test.shape}')\n",
        "print(f'y_test: {y_test.shape}')\n",
        "\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}